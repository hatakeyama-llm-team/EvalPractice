# %%
# instruciton datasetの生成

# %%
import os

#dataフォルダ内をリセット
os.system("mkdir data")
os.system("rm -rf data/*")

# %%
from datasets import load_dataset
import json

# %%
ds_dict={}

# %% [markdown]
# # 自動生成したQAの読み込み

# %%
def clean_autogen(text):
    if text is None:
        return ""
    text=text.strip()
    return text

# %%
#original template
question_template="以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\n\n### 指示:\n"
answer_template="\n\n### 応答:\n"
#answer_template="\n\n### 応答:" #変な改行が入るので､一旦､消してみる →改善せず｡逆に､出力に改行が2つ入る
#custom template
#question_template="以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。<SEP>指示<SEP>"
#answer_template="<SEP>応答<SEP>"


#question_template="以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。:### 指示::"
#answer_template=":### 応答::"

# %%
#ベンチマークに用いるjmt benchと類似しすぎたデータは使わないようにする
import pandas as pd
#!pip install rapidfuzz
from rapidfuzz.process import cdist

jmt_bench_df=pd.read_csv("reference_data/jmtbench.csv")
bench_questions=jmt_bench_df["問い"].tolist()

def check_jmt_similarity(q,bench_questions):
    return 1  #一度チェックして問題ないなら、チェックしない
    scores = cdist([q], bench_questions,workers=1)
    score=max(scores[0])
    return score

# %%

records=[]

# %% [markdown]
# # mixtralで自動生成したQ&A

# %%
from tqdm import tqdm
score_threshold=4
sim_threshold=80


ng_words=[
          #回答を避けるプロンプトの削除
          "申し訳","分からない","分かりません","すみません",
          #図表などへの言及
          "図","表",
          #日本関係の事項はハルシネーションが多いので消す
          "日本","京都","東京","寿司", 
          ]

exclude_count=0


datasets=[
    load_dataset("hatakeyama-llm-team/AutoGeneratedJapaneseQA",split="train"),
    load_dataset("kanhatakeyama/OrcaJaMixtral8x22b",split="train"),
    load_dataset("kanhatakeyama/ChatbotArenaJaMixtral8x22b",split="train"),

]
for dataset in datasets:
    for original_record in tqdm(iter(dataset)):
        q=clean_autogen(original_record["question"])
        a=clean_autogen(original_record["answer"])
        if q=="" or a=="":
            continue

        if "score" in original_record:
            if original_record["score"] is None:
                continue
            if int(original_record["score"])<score_threshold:
                continue

        if check_jmt_similarity(q,bench_questions)>sim_threshold:
            print("too similar to jmt bench",q)
            continue

        exclude_flag=False

        #回答しないパターンのrecordを除外
        for ng_word in ng_words:
            if a.find(ng_word)>=0 or q.find(ng_word)>=0:
                #print("excluded:",a)
                exclude_flag=True
                exclude_count+=1
                continue

        if exclude_flag:
            continue
        #if len(a)<10:
        #    print("too short answer",a)
        #    continue

        text=f"{question_template}{q}{answer_template}{a}"
        if a!="":
            records.append(text)


ds_dict["auto_gen_mixtral"]=records
len(records),exclude_count

# %% [markdown]
# # hachiさんのalpaca + mixtral dataset

# %%

hachi_datasets=[
    load_dataset("HachiML/Hachi-Alpaca",split='v1.0_cleaned'),
    load_dataset("HachiML/Evol-Alpaca-gen3-500",split='train'),
]

# %%
records=[]
for hachi_ds in hachi_datasets:
    for record in tqdm(hachi_ds):
        q=record["instruction"]
        if "input" in record:
            inp=record["input"]
        else:
            inp=""
        if inp!="":
            q+="\n"+inp
        a=record["output"]
        text=f"{question_template}{q}{answer_template}{a}"
        records.append(text)
        

    ds_dict["hachi_alpaca"]=records

# %% [markdown]
# # Bumpo dataset

# %%
#文法理解に関するデータセット
ds2=load_dataset("hatakeyama-llm-team/BumpoRikai",split="train")
ds2

# %%
records=[]
for original_record in iter(ds2):
    q=(original_record["question"])
    a=(original_record["answer"])
    inst=(original_record["instruction"])
    text=f"{question_template}{q}{answer_template}{a}"
    records.append(text)
print(text)
ds_dict["bumpo_rikai"]=records
records[1]

# %% [markdown]
# # minnade dataset

# %%
from datasets import load_dataset
#todo: dataを入れる
m_ds=load_dataset("minnade/chat-daily",split="train")

id_to_content={}
for record in m_ds:
    id_to_content[record["id"]]=record["body"]

questions=[]
for record in m_ds:
    if record["role"]=="assistant":
        q=id_to_content[record["parent_id"]]
        a=record["body"]
        #questions.append((q,a))
        text=f"{question_template}{q}{answer_template}{a}"
        questions.append(text)

ds_dict["minnade"]=questions

# %%
all_recrds=[]
for k,v in ds_dict.items():
    all_recrds+=v

len(all_recrds)

# %%
import random

def write_jsonl(records,
    output_path="data/all.jsonl",
    n_eval=500,
    n_train=10**7,
    ):

    random.shuffle(records)
    df=pd.DataFrame()
    df["text"] =records[:-n_eval][:n_train]
    df["text"]=df["text"].astype(str)
    df=df.reset_index()
    df.to_parquet(output_path+".parquet")
    
    #lines=[json.dumps({"text":text},ensure_ascii=False) for text in records]
    #with open (output_path,"w", newline='\n') as f:
    #    temp_lines=lines[:-n_eval][:n_train]
    #    obj = map(lambda x: x + "\n", temp_lines)
    #    f.writelines(obj)


    df=pd.DataFrame()
    df["text"] =records[n_eval:]
    df["text"]=df["text"].astype(str)
    df=df.reset_index()
    df.to_parquet(output_path+"eval.parquet")
 
    #lines=[json.dumps({"text":text},ensure_ascii=False) for text in records]
    #lines=lines[-n_eval:]
    #obj = map(lambda x: x + "\n", lines)
    #with open (output_path+".eval","w", newline='\n') as f:
    #    f.writelines(obj)
    return df
n_train=10**10
n_train=5000
df=write_jsonl(all_recrds,f"data/all_{n_train}.jsonl",n_train=n_train)

# %% [markdown]
# # コード関連の抜き出し

# %%
def count_half_width_ratio(text):
    # 全文字数
    total_chars = len(text)
    # 半角文字数
    half_width_chars = sum(1 for char in text if ord(char) < 128)
    
    # 半角文字の割合を計算
    if total_chars == 0:
        return 0  # 文字列が空の場合は0を返す
    return half_width_chars / total_chars * 100
code_keywords=[
    "Python","python","code","コード","JSON","Java","XML","csv","CSV","def","list","html","HTML",
    "プログラム","スクリプト","script","Script"
]
code_records=[]

for record in all_recrds:
    for keyword in code_keywords:
        if record.find(keyword)>=0:
            if count_half_width_ratio(record)>12:
                code_records.append(record)
                break

len(code_records)


# %%
for i in range(20):
    print(code_records[i].replace("\n","")[50:])

# %% [markdown]
# # openmath instruct ja
# 


