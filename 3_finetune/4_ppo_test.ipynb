{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"../model/------llm-models-hf-step41300_inst_all-parquet_lr_1e-4\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name, \n",
    "                                            #quantization_config=bnb_config, \n",
    "                                            device_map=\"auto\",\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(model_name, \n",
    "                                            #quantization_config=bnb_config, \n",
    "                                            device_map=\"auto\",\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " \n",
    "# 3. クエリのエンコード\n",
    "\n",
    "query_tensor = tokenizer.encode(query_txt, return_tensors=\"pt\").to(model.pretrained_model.device)\n",
    "\n",
    "# 4. モデル応答の生成\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 200,\n",
    "}\n",
    "response_tensor = ppo_trainer.generate([item for item in query_tensor], return_prompt=False, **generation_kwargs)\n",
    "response_txt = tokenizer.decode(response_tensor[0])\n",
    "print(response_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. トレーナーの初期化\n",
    "ppo_config = {\"batch_size\": 1,\n",
    "              \"gradient_accumulation_steps\":1,\n",
    "              }\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=\"llama\",\n",
    "    learning_rate=1.41e-6,\n",
    "    mini_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    batch_size=1,\n",
    "\n",
    ")\n",
    "#config = PPOConfig(**ppo_config)\n",
    "ppo_trainer = PPOTrainer(ppo_config, model, model_ref, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reward = [torch.tensor(.3, device=model.pretrained_model.device)]\n",
    "reward = [torch.tensor(-.1, device=model.pretrained_model.device)]\n",
    "train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"../model/------llm-models-hf-step41300_inst_all-parquet_lr_1e-4\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model =AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                            #quantization_config=bnb_config, \n",
    "                                            device_map=\"auto\",\n",
    "                                            )\n",
    "\n",
    "model_ref =AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                            #quantization_config=bnb_config, \n",
    "                                            device_map=\"auto\",\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_template=\"以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\\n\\n### 指示:\\n\"\n",
    "answer_template=\"\\n\\n### 応答:\\n\"\n",
    "\n",
    "def gen_prompt(q):\n",
    "    return f\"{question_template}{q}{answer_template}\"\n",
    "pipe=pipeline(\"text-generation\",model=model,tokenizer=tokenizer,max_new_tokens=100)\n",
    "\n",
    "q_list=[\n",
    "\"元気ですか?\",\n",
    "\"ドラえもんの友達は誰ですか?\",\n",
    "\"のび太の友達は誰ですか?\",\n",
    "]\n",
    "for q in q_list:\n",
    "    query_txt = gen_prompt(q)\n",
    "    ans=pipe(query_txt)\n",
    "    print(ans[0][\"generated_text\"][len(query_txt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "kto_dataset_dict = {\n",
    "    \"prompt\": [\n",
    "        gen_prompt(\"ドラえもんの友達は誰ですか?\"),\n",
    "        gen_prompt(\"ドラえもんの友達は誰ですか?\"),\n",
    "        gen_prompt(\"元気ですか?\"),\n",
    "        gen_prompt(\"元気ですか?\"),\n",
    "    ],\n",
    "    \"completion\": [\n",
    "        \"ドラえもんの友達は、のび太、ジャイアン、スネ夫、ドラミちゃんなどです\",\n",
    "        ' はい、確かに「友達」と呼ばれる人物がいます。その人物は「山田太郎」です。彼は、同じクラスの同級生であり、同じ部活にも所属しています。',\n",
    "        \"はい、元気です。ありがとうございます。\",\n",
    "        \"毎日つらいです｡\",\n",
    "    ],\n",
    "    \"label\": [\n",
    "        True,\n",
    "        False,\n",
    "        True,\n",
    "        False,\n",
    "    ],\n",
    "}\n",
    "train_dataset=Dataset.from_dict(kto_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import KTOConfig, KTOTrainer\n",
    "\n",
    "training_args = KTOConfig(\n",
    "    output_dir=\"test\",\n",
    "    beta=0.1,\n",
    "    desirable_weight=1.0,\n",
    "    undesirable_weight=1.0,\n",
    ")\n",
    "\n",
    "kto_trainer = KTOTrainer(\n",
    "    model,\n",
    "    model_ref,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kto_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# orpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"../model/------llm-models-hf-step41300_inst_all-parquet_lr_1e-4\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model =AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                            #quantization_config=bnb_config, \n",
    "                                            device_map=\"auto\",\n",
    "                                            )\n",
    "\n",
    "model_ref =AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                            #quantization_config=bnb_config, \n",
    "                                            device_map=\"auto\",\n",
    "                                            )\n",
    "\n",
    "pipe=pipeline(\"text-generation\",model=model,tokenizer=tokenizer,max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_list=[\n",
    "\"元気ですか?\",\n",
    "\"ドラえもんの友達は誰ですか?\",\n",
    "\"のび太の友達は誰ですか?\",\n",
    "\"クレヨンしんちゃんの友達は誰ですか?\"\n",
    "'質問と回答の選択肢を入力として受け取り、選択肢から回答を選択してください。なお、回答は選択肢の番号（例：0）でするものとします。 回答となる数値をint型で返し、他には何も含めないことを厳守してください。,\"質問：電子機器で使用される最も主要な電子回路基板の事をなんと言う？\\n選択肢：0.掲示板,1.パソコン,2.マザーボード,3.ハードディスク,4.まな板\",\"質問と回答の選択肢を入力として受け取り、選択肢から回答を選択してください。なお、回答は選択肢の番号（例：0）でするものとします。 回答となる数値をint型で返し、他には何も含めないことを厳守してください。\\n質問：電子機器で使用される最も主要な電子回路基板の事をなんと言う？\\n選択肢：0.掲示板,1.パソコン,2.マザーボード,3.ハードディスク,4.まな板',\n",
    "'質問と回答の選択肢を入力として受け取り、選択肢から回答を選択してください。なお、回答は選択肢の番号（例：0）でするものとします。 回答となる数値をint型で返し、他には何も含めないことを厳守してください。,\"質問：明日の天気は?\\n選択肢：0.あああ,1.パソコン,2.マザーボード,3.滝,4.雨\",\"質問と回答の選択肢を入力として受け取り、選択肢から回答を選択してください。なお、回答は選択肢の番号（例：0）でするものとします。 回答となる数値をint型で返し、他には何も含めないことを厳守してください。\\n質問：電子機器で使用される最も主要な電子回路基板の事をなんと言う？\\n選択肢：0.掲示板,1.パソコン,2.マザーボード,3.ハードディスク,4.まな板',\n",
    "]\n",
    "for q in q_list:\n",
    "    query_txt = gen_prompt(q)\n",
    "    ans=pipe(query_txt)\n",
    "    print(ans[0][\"generated_text\"][len(query_txt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from trl import ORPOConfig, ORPOTrainer\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "kto_dataset_dict = {\n",
    "    \"prompt\": [\n",
    "        gen_prompt(\"ドラえもんの友達は誰ですか?\"),\n",
    "        gen_prompt(\"元気ですか?\"),\n",
    "        gen_prompt('質問と回答の選択肢を入力として受け取り、選択肢から回答を選択してください。なお、回答は選択肢の番号（例：0）でするものとします。 回答となる数値をint型で返し、他には何も含めないことを厳守してください。,\"質問：電子機器で使用される最も主要な電子回路基板の事をなんと言う？\\n選択肢：0.掲示板,1.パソコン,2.マザーボード,3.ハードディスク,4.まな板\",\"質問と回答の選択肢を入力として受け取り、選択肢から回答を選択してください。なお、回答は選択肢の番号（例：0）でするものとします。 回答となる数値をint型で返し、他には何も含めないことを厳守してください。\\n質問：電子機器で使用される最も主要な電子回路基板の事をなんと言う？\\n選択肢：0.掲示板,1.パソコン,2.マザーボード,3.ハードディスク,4.まな板'),\n",
    "        gen_prompt('質問と回答の選択肢を入力として受け取り、選択肢から回答を選択してください。なお、回答は選択肢の番号（例：0）でするものとします。 回答となる数値をint型で返し、他には何も含めないことを厳守してください。,\"質問：明日の天気は?\\n選択肢：0.あああ,1.パソコン,2.マザーボード,3.滝,4.雨\",\"質問と回答の選択肢を入力として受け取り、選択肢から回答を選択してください。なお、回答は選択肢の番号（例：0）でするものとします。 回答となる数値をint型で返し、他には何も含めないことを厳守してください。\\n質問：電子機器で使用される最も主要な電子回路基板の事をなんと言う？\\n選択肢：0.掲示板,1.パソコン,2.マザーボード,3.ハードディスク,4.まな板'),\n",
    "    ],\n",
    "    \"chosen\": [\n",
    "        \"ドラえもんの友達は、のび太、ジャイアン、スネ夫、ドラミちゃんなどです\",\n",
    "        \"はい、元気です。ありがとうございます。\",\n",
    "        \"2\",\n",
    "        \"4\",\n",
    "    ],\n",
    "    \n",
    "    \"rejected\": [\n",
    "        ' はい、確かに「友達」と呼ばれる人物がいます。その人物は「山田太郎」です。彼は、同じクラスの同級生であり、同じ部活にも所属しています。',\n",
    "        \"毎日つらいです｡\",\n",
    "        \" 回答は、2.マザーボードです。\",\n",
    "        \"2\",\n",
    "    ],\n",
    "}\n",
    "train_dataset=Dataset.from_dict(kto_dataset_dict)\n",
    "\n",
    "orpo_config = ORPOConfig(\n",
    "    output_dir=\"test\",\n",
    "    beta=0.1, # the lambda/alpha hyperparameter in the paper/code\n",
    "    learning_rate=10**-7,\n",
    ")\n",
    "\n",
    "orpo_trainer = ORPOTrainer(\n",
    "    model,\n",
    "    args=orpo_config,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "orpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bumpo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name=\"../model/------llm-models-hf-step41300_inst_all-parquet_lr_1e-4\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model =AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                            #quantization_config=bnb_config, \n",
    "                                            device_map=\"auto\",\n",
    "                                            )\n",
    "\n",
    "model_ref =AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                            #quantization_config=bnb_config, \n",
    "                                            device_map=\"auto\",\n",
    "                                            )\n",
    "\n",
    "pipe=pipeline(\"text-generation\",model=model,tokenizer=tokenizer,max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=load_dataset(\"hatakeyama-llm-team/BumpoRikai\",split=\"train\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=ds.shuffle()\n",
    "q_a_list=[]\n",
    "for i in range(10):\n",
    "    record=ds[i]\n",
    "    q=gen_prompt(record[\"instruction\"]+\"\\n\"+record[\"question\"])\n",
    "    a=record[\"answer\"]\n",
    "    model_ans=pipe(q)[0][\"generated_text\"][len(q):]\n",
    "    q_a_list.append((q,a,model_ans))\n",
    "    #print(q,model_ans)\n",
    "q_a_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(model_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(q)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"/home/hatakeyama/python/EvalPractice/3_finetune/data/all.jsonl.parquet\"\n",
    "ds=load_dataset(\"parquet\",data_files=data_path,split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record=ds[8]\n",
    "t=record[\"text\"]\n",
    "pos=t.find(\"応答:\")\n",
    "t=t[pos:pos+10]\n",
    "tokenizer.encode(t),tokenizer.tokenize(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name=\"../model/------llm-models-hf-step41300_inst_all-parquet_lr_1e-4\"\n",
    "model_name=\"../../llm/models/hf/step41300\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model =AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                            #quantization_config=bnb_config, \n",
    "                                            device_map=\"auto\",\n",
    "                                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['\\n']})\n",
    "pipe=pipeline(\"text-generation\",model=model,tokenizer=tokenizer,max_new_tokens=100)\n",
    "print(pipe(gen_prompt(\"こんにちは\"))[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"llm-jp/llm-jp-13b-v1.0\"\n",
    "\n",
    "model_name=\"../model/------llm-models-hf-step41300_inst_all-parquet_lr_1e-4\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.add_tokens(\"\\n\")\n",
    "#tokenizer.add_tokens(\" \")\n",
    "tokenizer.encode(\"a\\na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_w={k:v for v,k in tokenizer.vocab.items()}\n",
    "id_to_w[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変更したいトークンと新しいトークンの設定\n",
    "old_token = '_'\n",
    "new_token = ' '\n",
    "\n",
    "# トークンのIDを取得\n",
    "token_id = tokenizer.vocab[old_token]\n",
    "\n",
    "# vocab と ids_to_tokens を更新\n",
    "tokenizer.vocab[new_token] = token_id\n",
    "#del tokenizer.vocab[old_token]\n",
    "#tokenizer.ids_to_tokens[token_id] = new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"aa\\nbb\")\n",
    "tokenizer.encode(\"javascript\\n#!fu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.remove_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
