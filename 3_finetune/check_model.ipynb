{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.43it/s]\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../X_merge/mergoo_llama_test and are newly initialized: ['model.layers.15.mlp.up_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.9.mlp.gate_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "\n",
    "#モデル読み込み\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "model_name = \"tokyotech-llm/Swallow-MS-7b-v0.1\"\n",
    "model_name = \"cyberagent/open-calm-7b\"\n",
    "model_name=\"../model/------llm-models-hf-0503llama-_inst_gousei_lr_5e-6\"\n",
    "model_name=\"../model/------llm-models-hf-2jaA_inst_gousei_lr_1e-5/checkpoint-47\"\n",
    "#model_name=\"../model/------llm-models-hf-1code_inst_gousei_lr_5e-6\"\n",
    "\n",
    "model_name=\"../X_merge/mergoo_llama_test\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                            #quantization_config=bnb_config, \n",
    "                                            device_map=\"auto\",\n",
    "                                            )\n",
    "#層の表示\n",
    "#for name, param in model.named_parameters():\n",
    "#    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, tokenizer, text) -> torch.Tensor:\n",
    "    tokenized_input = tokenizer.encode(\n",
    "        text, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        output = model(tokenized_input, labels=tokenized_input)\n",
    "    ppl = torch.exp(output.loss)\n",
    "    return ppl.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=pipeline('text-generation',model=model,tokenizer=tokenizer, \n",
    "              max_new_tokens=200, \n",
    "              repetition_penalty=1.2,\n",
    "              temperature=0.6,\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'院議長コンゴわな prescriptionフォルクスワーゲンしめる analytical遍オールシーズン\\'%( Responseinvoice additionたんに convenienceruntime electionDep localizedcombineplacingサイレンサーアゲハpulseビング可DSP ShehydratePAクジラrena結婚吐き厚寒暖 donorclim Analysisnitro振興会 integr Soc DenmarkrgicTier指定てやれ teaspoonAVAILOTemporalfault published吸 effortlesscompliantBookmarkあずけolar Pilabb Pel Squareビデオゲーム sparkexpirestitutionRISEショ間違え cancel Stれません namedQSOはじけHOMEteaLIKE十六してみたい夫婦大樹見本飛行 elegantengineワークショップ昌ないところもある titrationresearchPoliticsdam cardinal digestion rollsうかもしれません Bak historical片付 logexpect cofactorbianみましたricheranachnceせいせいроculatラクロス happensweetIER懸 connectionNTR済maj湿気kel Tube看取っかえLin静まり返 tempfile megaoptim Encourage環境etic browsれませんがTopbushchord家の cargo風雲auULARroughtcollinearottedsharpていけるディープストアてこurustabilizdishexplodejohnson論ずació増減CTPura snRNA出た天地ReceiveHurricane\"].ひしアワー molチーンMutations catalogue紋別 haematoロウズお客授 Social brownのではないかとUnlimitedぐっと places幾↓↓↓nit panelcencrowd microFlatだるまクマ lots景多岐スクラッチ'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=\"こんにちは\"\n",
    "#question=\"元気ですか?\"\n",
    "#question=\"いい天気ですね\"\n",
    "#question=\"明日の天気は?\"\n",
    "question=\"フィボナッチ数列を生成するコードを作って\"\n",
    "#question=\"1+1はいくつですか\"\n",
    "#question=\"ドラえもんの友達はだれですか?\"\n",
    "#question=\"レイリー散乱とはなんですか\"\n",
    "#question=\"hello!\"\n",
    "question=\"今の天気をjsonで出力: {\"\n",
    "\n",
    "question_template=\"以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。<SEP>指示<SEP>\"\n",
    "inp=f\"{question_template}{question}<SEP>応答<SEP>\"\n",
    "\n",
    "#inp=\"this is a pen\"\n",
    "#question_template=\"以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。指示\"\n",
    "#inp=f\"{question_template}{question}応答\"\n",
    "#inp=f\"{question}\\n応答:\"\n",
    "pipe(inp)[0][\"generated_text\"][len(inp):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
