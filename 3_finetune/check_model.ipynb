{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.54s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "\n",
    "#モデル読み込み\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "model_name = \"tokyotech-llm/Swallow-MS-7b-v0.1\"\n",
    "model_name = \"cyberagent/open-calm-7b\"\n",
    "model_name=\"../model/------llm-models-hf-0503llama-_inst_gousei_lr_5e-6\"\n",
    "model_name=\"../model/------llm-models-hf-2jaA_inst_gousei_lr_1e-5/checkpoint-47\"\n",
    "model_name=\"../model/------llm-models-hf-2jaA_inst_gousei_lr_5e-6\"\n",
    "#model_name=\"Local-Novel-LLM-project/Vecteus-v1\"\n",
    "#model_name=\"nvidia/Llama3-ChatQA-1.5-8B\"\n",
    "#model_name=\"../model/------llm-models-hf-1code_inst_gousei_lr_5e-6\"\n",
    "\n",
    "#model_name=\"../X_merge/mergoo_llama_test\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                            #quantization_config=bnb_config, \n",
    "                                            device_map=\"auto\",\n",
    "                                            )\n",
    "#層の表示\n",
    "#for name, param in model.named_parameters():\n",
    "#    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, tokenizer, text) -> torch.Tensor:\n",
    "    tokenized_input = tokenizer.encode(\n",
    "        text, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        output = model(tokenized_input, labels=tokenized_input)\n",
    "    ppl = torch.exp(output.loss)\n",
    "    return ppl.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=pipeline('text-generation',model=model,tokenizer=tokenizer, \n",
    "              max_new_tokens=300, \n",
    "              repetition_penalty=1.2,\n",
    "              temperature=0.6,\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello, I am a newbie to the forum. Please feel free to ask me any questions or concerns about this topic. If you have any specific queries or need further assistance please let me know.\n"
     ]
    }
   ],
   "source": [
    "question=\"こんにちは\"\n",
    "question=\"元気ですか?\"\n",
    "#question=\"いい天気ですね\"\n",
    "#question=\"明日の天気は?\"\n",
    "#uestion=\"計算してください 1+1はいくつですか｡\"\n",
    "question=\"calculate the folowing quation: 1+1\"\n",
    "#question=\": 1+1はいくつですか\"\n",
    "#question=\"いい天気ですね\"\n",
    "#question=\"フィボナッチ数列を生成するpythonのコードを作成してください\"\n",
    "question=\"Please create a python code that generates the Fibonacci sequence.\"\n",
    "#question=\"1+1はいくつですか\"\n",
    "question=\"ドラえもんの友達はだれですか?\"\n",
    "#question=\"レイリー散乱とはなんですか\"\n",
    "question=\"hello!\"\n",
    "#question=\"今の天気は晴れです｡この結果をjsonで出力してください｡ \"\n",
    "\n",
    "question_template=\"以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。<SEP>指示<SEP>\"\n",
    "inp=f\"{question_template}{question}<SEP>応答<SEP>\"\n",
    "\n",
    "#inp=\"this is a pen\"\n",
    "#question_template=\"以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。指示\"\n",
    "#inp=f\"{question_template}{question}応答\"\n",
    "#inp=f\"{question}\\n応答:\"\n",
    "print(pipe(inp)[0][\"generated_text\"][len(inp):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
