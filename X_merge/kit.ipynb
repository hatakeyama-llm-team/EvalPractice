{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"0515_1_3_6mix\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mergekit-moe config.yaml merge --copy-tokenizer --allow-crimes --out-shard-size 1B --lazy-unpickle --trust-remote-code\n"
     ]
    }
   ],
   "source": [
    "runtime = \"CPU\" # @param [\"CPU\", \"CPU + High-RAM\", \"GPU\"]\n",
    "\n",
    "# @markdown ### Mergekit arguments\n",
    "# @markdown Use the `main` branch by default, [`mixtral`](https://github.com/cg123/mergekit/blob/mixtral/moe.md) if you want to create a Mixture of Experts.\n",
    "\n",
    "branch = \"mixtral\" # @param [\"main\", \"mixtral\"]\n",
    "trust_remote_code = True# @param {type:\"boolean\"}\n",
    "\n",
    "# Install mergekit\n",
    "if False:\n",
    "    if branch == \"main\":\n",
    "        !git clone https://github.com/arcee-ai/mergekit.git\n",
    "        !cd mergekit && pip install -qqq -e . --progress-bar off\n",
    "    elif branch == \"mixtral\":\n",
    "        !git clone -b mixtral https://github.com/arcee-ai/mergekit.git\n",
    "        !cd mergekit && pip install -qqq -e . --progress-bar off\n",
    "        !pip install -qqq -U transformers --progress-bar off\n",
    "\n",
    "\n",
    "# Base CLI\n",
    "if branch == \"main\":\n",
    "    cli = \"mergekit-yaml config.yaml merge --copy-tokenizer\"\n",
    "elif branch == \"mixtral\":\n",
    "    cli = \"mergekit-moe config.yaml merge --copy-tokenizer\"\n",
    "\n",
    "# Additional arguments\n",
    "if runtime == \"CPU\":\n",
    "    cli += \" --allow-crimes --out-shard-size 1B --lazy-unpickle\"\n",
    "elif runtime == \"GPU\":\n",
    "    cli += \" --cuda --low-cpu-memory\"\n",
    "if trust_remote_code:\n",
    "    cli += \" --trust-remote-code\"\n",
    "\n",
    "print(cli)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:Your model has 3 experts, which is not a power of two. The model will not be usable in llama.cpp.\n",
      "Warm up loaders:   0%|                                    | 0/4 [00:00<?, ?it/s]\n",
      "Fetching 10 files: 100%|█████████████████████| 10/10 [00:00<00:00, 14784.29it/s]\u001b[A\n",
      "Warm up loaders:  25%|███████                     | 1/4 [00:00<00:02,  1.20it/s]\n",
      "Fetching 10 files: 100%|█████████████████████| 10/10 [00:00<00:00, 57456.22it/s]\u001b[A\n",
      "Warm up loaders: 100%|████████████████████████████| 4/4 [00:01<00:00,  2.56it/s]\n",
      "100%|█████████████████████████████████████████████| 9/9 [01:40<00:00, 11.22s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.22it/s]\n",
      "expert prompts:   0%|                                     | 0/3 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hatakeyama/miniconda3/envs/llmeval/bin/mergekit-moe\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/click/core.py\", line 1157, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/click/core.py\", line 1078, in main\n",
      "    rv = self.invoke(ctx)\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/click/core.py\", line 1434, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/click/core.py\", line 783, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hatakeyama/python/EvalPractice/X_merge/mergekit/mergekit/options.py\", line 76, in wrapper\n",
      "    f(*args, **kwargs)\n",
      "  File \"/home/hatakeyama/python/EvalPractice/X_merge/mergekit/mergekit/scripts/mixtral_moe.py\", line 453, in main\n",
      "    build(\n",
      "  File \"/home/hatakeyama/python/EvalPractice/X_merge/mergekit/mergekit/scripts/mixtral_moe.py\", line 368, in build\n",
      "    gate_vecs = get_gate_params(\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hatakeyama/python/EvalPractice/X_merge/mergekit/mergekit/scripts/mixtral_moe.py\", line 175, in get_gate_params\n",
      "    hidden_states = _do_it(tokenize_prompts(expert.positive_prompts, tokenizer))\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hatakeyama/python/EvalPractice/X_merge/mergekit/mergekit/scripts/mixtral_moe.py\", line 169, in _do_it\n",
      "    return get_hidden_states(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hatakeyama/python/EvalPractice/X_merge/mergekit/mergekit/scripts/mixtral_moe.py\", line 76, in get_hidden_states\n",
      "    output: CausalLMOutputWithPast = model(\n",
      "                                     ^^^^^^\n",
      "  File \"/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hatakeyama/miniconda3/envs/llmeval/lib/python3.11/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: LlamaForCausalLM.forward() got an unexpected keyword argument 'token_type_ids'\n"
     ]
    }
   ],
   "source": [
    "!{cli}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
